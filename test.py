from Vocabularizer import Tokenize

print(Tokenize("Hello, my name is Ming. How are you doing today? The weather is fantastic.", ["sometimes ", " technolog", "jamie: ", "people ", "ant to ", "really ", ": that ", "about ", "s and ", "alex: ", ", and ", "lily: ", ": that", "emma: ", "dr. ch", ": yeah", " thing", "thing ", "helps ", "import", " that ", "anna: ", ", but ", "maya: ", "ounds ", " the ", "like ", "it\u2019s ", "have ", "what ", "learn", " that", "king ", "alice", ": yes", "feel ", "max: ", "about", "scien", " and ", "frien", "ben: ", "ther ", "make ", "every", "just ", "when ", "es.\n\n", "with ", "s.\n\n", ": i ", "you ", "e.\n\n", "how ", "ore ", "jake", "ood ", " the", "ind ", "stor", "are ", "i\u2019m ", " tru", "work", "ther", "for ", "ough", "new ", "one ", "chan", "idea", ", i ", "book", "keep", "ere ", "expl", "ure ", "?\n\n", "is ", "it ", "e t", " to", "an ", "be ", "gre", "s m", "and", "ent", "e c", "do ", "e. ", "ay ", "\u2019t ", "ce ", "on ", "es ", "s. ", "way", "now", "so ", "of ", "art", "ic ", "!\n\n", "ld ", "ool", "der", "pro", "y. ", "ast", "ome", "bob", "i w", "rea", "ed ", "ure", "as ", "ack", "s, ", "us ", "day", "we ", " wh", "fun", "uch", "ind", "in ", "get", "fer", "om ", "har", "nat", "a ", " s", "is", " m", "! ", "be", "ed", "av", "he", "ul", "se", "il", "sh", "om", "ri", "if", "ir", "tt", "gr", "bi", "sp", "as", "go", "jo", "aw", "mo", "gu", "ap", "ph", "cl", "q", "b", ".", "e", "!", "\u201c", "n", "u", "\u2019", ":", "k", " ", "w", "l", "v", "p", "c", "h", "m", "i", "o", "s", "g", "r", "f", "\n", "\u201d", "-", "a", "x", "d", "z", ",", "y", "?", "\u2014", "j", "t", "<empty>", "<unk>"], 20))
